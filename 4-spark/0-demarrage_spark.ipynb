{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q469u8ZokcRS"
      },
      "source": [
        "# Installation de Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRQ3s5YCHO9s",
        "outputId": "40531862-9686-44a4-df11-7bc4f13a5b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/bin/sh\n",
            "\n",
            "# This script intends to install Spark on Ubuntu.\n",
            "# It works well on :\n",
            "# - Ubuntu 22.04 LTS\n",
            "# - Google Colab (based on Ubuntu 22.04 LTS)\n",
            "\n",
            "# Spark requires Java 8 or later, but soon, Java version prior to 17 will be deprecated.\n",
            "sudo apt-get update -y -q\n",
            "sudo apt-get install openjdk-17-jdk-headless -y -q\n",
            "\n",
            "# Download Spark. You can check the latest version of Spark at https://spark.apache.org/downloads.html.\n",
            "SPARK_VERSION=3.5.5\n",
            "wget -nc -q https://dlcdn.apache.org/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
            "tar xf spark-$SPARK_VERSION-bin-hadoop3.tgz\n",
            "\n",
            "# Install findspark and pyspark to use Spark with Python.\n",
            "pip install -q findspark\n",
            "pip install pyspark==$SPARK_VERSION\n"
          ]
        }
      ],
      "source": [
        "# Téléchargement du script d'installation\n",
        "!wget -nc -q https://raw.githubusercontent.com/bamedro/training-bigdata/main/4-spark/setup-spark-on-ubuntu.sh\n",
        "!chmod u+x setup-spark-on-ubuntu.sh\n",
        "!cat setup-spark-on-ubuntu.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FID1ECxDj6Zx"
      },
      "outputs": [],
      "source": [
        "# Lancerment du script d'installation\n",
        "!./setup-spark-on-ubuntu.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZWkoCu5nAbk"
      },
      "outputs": [],
      "source": [
        "# Définition des variables d'environnement (adapter si besoin)\n",
        "!ls /usr/lib/jvm/ /content\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXDJ7WAOn4sx"
      },
      "outputs": [],
      "source": [
        "# Pour vérification\n",
        "os.environ[\"SPARK_HOME\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BeGOBDSycvP"
      },
      "source": [
        "# Créer un SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "0XoYFi-eoCty",
        "outputId": "c05199d7-3417-4042-9405-ed12b6193d24"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2db728a43b59:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>First Context</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=First Context>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import de SparkContext du module pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Définition d'un SparkContext en local\n",
        "conf = SparkConf().setAppName(\"First Context\")\n",
        "sc = SparkContext('local', conf=conf)\n",
        "sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83pQDWBz4aEW",
        "outputId": "df65ef33-70d4-41b9-c286-75f3aea01aa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\t\t       spark-3.5.5-bin-hadoop3.tgz\n",
            "setup-spark-on-ubuntu.sh       Twain-HF.txt\n"
          ]
        }
      ],
      "source": [
        "# Télechargement d'un texte d'exemple\n",
        "!wget -nc -q https://raw.githubusercontent.com/bamedro/training-bigdata/main/4-spark/Twain-HF.txt\n",
        "!ls /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUtTufQFzI0h",
        "outputId": "5f7fad71-4769-44c3-95fb-30fe13d5cef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Réalisé en 0.064 secondes\n"
          ]
        }
      ],
      "source": [
        "# Importer la bibliothèque time et calcul du temps au début de l'exécution (t0)\n",
        "from time import time\n",
        "t0 = time()\n",
        "# Importer la base de données \"Twain-HF.txt\" dans un RDD appelé txt_rdd\n",
        "txt_rdd = sc.textFile(\"/content/Twain-HF.txt\")\n",
        "# Calcul du temps de la lecture du fichier\n",
        "t1 = time() - t0\n",
        "print(\"Réalisé en {} secondes\".format(round(t1,3)))\n",
        "#txt_rdd.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHlVCsdOU-i9",
        "outputId": "96168b87-0b1c-49cc-e7fd-7d29b0ec9cce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Twain-HF.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt_rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT4l__b72gmP",
        "outputId": "5e4f3c26-aad5-4455-917f-e49e3be2f10b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4448"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt_rdd.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd2jb7M0lN8F"
      },
      "source": [
        "# Fermer sc, le SparkContext de l'exercice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKWCnCpK1WfQ"
      },
      "outputs": [],
      "source": [
        "sc.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
